---
title: "Choosing replications"
author: "Amy Heather"
date: "`r Sys.Date()`"
output:
  github_document:
      toc: true
      html_preview: false
---

This notebook documents the choice of the number of replications.

The generated images are saved and then loaded, so that we view the image as saved (i.e. with the dimensions set in `ggsave()`). This also avoids the creation of a `_files/` directory when knitting the document (which would save all previewed images into that folder also, so they can be rendered and displayed within the output `.md` file, even if we had not specifically saved them). These are viewed using `include_graphics()`, which must be the last command in the cell (or last in the plotting function).

Some of these figures are used in the paper (`mock_paper.md`) - see below:

* **Figure C.1:** `outputs/reps_algorithm_wait_time.png`
* **Figure C.2:** `outputs/reps_algorithm_serve_time.png`
* **Figure C.3:** `outputs/reps_algorithm_utilisation.png`

The run time is provided at the end of the notebook.

## Set up

Install the latest version of the local simulation package. If running sequentially, `devtools::load_all()` is sufficient. If running in parallel, you must use `devtools::install()`.

```{r}
devtools::load_all()
```

Load required packages.

```{r}
# nolint start: undesirable_function_linter.
library(data.table)
library(dplyr)
library(knitr)
library(simulation)
library(tidyr)

options(data.table.summarise.inform = FALSE)
options(dplyr.summarise.inform = FALSE)
# nolint end
```

Start timer.

```{r}
start_time <- Sys.time()
```

Define path to outputs folder.

```{r}
output_dir <- file.path("..", "outputs")
```

## Choosing the number of replications

The **confidence interval method** can help you decide how many replications (runs) your simulation needs. The more replications you run, the narrower your confidence interval becomes, leading to a more precise estimate of the model's mean performance.

There are two main calculations:

* **Confidence interval**. This is the range where the true mean is likely to be, based on your simulation results. For example, a 95% confidence interval means that, if you repeated the experiment many times, about 95 out of 100 intervals would contain the true mean.
* **Precision**. This tells you how close that range is to your mean. For example, if your mean is 50 and your 95% confidence interval is 45 to 55, your precision is Â±10% (because 5 is 10% of 50).

To run this method you:

* Run the model with more and more replications.
* Check after each how wide your confidence interval is.
* Stop when the interval is narrow enough to meet your desired precision.
* Make sure the interval stays this narrow if you keep running more replications.

This method is less useful for values very close to zero - so, for example, when using utilisation (which ranges from 0 to 1) it is recommended to multiple values by 100.

When deciding how many replications you need, repeat this process for each performance measure you care about, and use the largest number you find.

It's important to check ahead, to check that the 10% precision is maintained - which is fine in this case - it doesn't go back up to future deviation.

```{r}
# Run calculations and produce plot
ci_df <- confidence_interval_method(
  replications = 150L,
  desired_precision = 0.1,
  metric = "mean_serve_time_nurse"
)

# Preview dataframe
head(ci_df)

# View first ten rows where percentage deviation is below 5
ci_df %>%
  filter(deviation < 0.05) %>%
  head(10L)
```

```{r}
# Create plot
path <- file.path(output_dir, "conf_int_method_serve_time.png")
plot_replication_ci(
  conf_ints = ci_df,
  yaxis_title = "Mean time with nurse",
  file_path = path,
  min_rep = 86L
)
# View plot
include_graphics(path)
```

It is also important to check across multiple metrics.

```{r}
# Run calculations
ci_df <- confidence_interval_method(
  replications = 1000L,
  desired_precision = 0.1,
  metric = "mean_waiting_time_nurse"
)

# Preview dataframe
tail(ci_df)

# Create plot
path <- file.path(output_dir, "conf_int_method_wait_time.png")
plot_replication_ci(
  conf_ints = ci_df,
  yaxis_title = "Mean wait time for the nurse",
  file_path = path
)
# View plot
include_graphics(path)
```

```{r}
# Run calculations
ci_df <- confidence_interval_method(
  replications = 200L,
  desired_precision = 0.1,
  metric = "utilisation_nurse"
)

# Preview dataframe
head(ci_df)

# View first ten rows where percentage deviation is below 5
ci_df %>%
  filter(deviation < 0.05) %>%
  head(10L)

# Create plot
path <- file.path(output_dir, "conf_int_method_utilisation.png")
plot_replication_ci(
  conf_ints = ci_df,
  yaxis_title = "Mean nurse utilisation",
  file_path = path,
  min_rep = 151L
)
# View plot
include_graphics(path)
```

## Automated detection of the number of replications

Run the algorithm (which will run model with increasing reps) for a few different metrics.

```{r}
# Set up and run algorithm
alg <- ReplicationsAlgorithm$new(param = parameters())
alg$select()

# View results
alg$nreps
head(alg$summary_table)
```

Visualise results for each metric...

```{r}
path <- file.path(output_dir, "reps_algorithm_wait_time.png")
plot_replication_ci(
  conf_ints = filter(alg$summary_table, metric == "mean_waiting_time_nurse"),
  yaxis_title = "Mean wait time for nurse",
  file_path = path
)
include_graphics(path)
```

```{r}
path <- file.path(output_dir, "reps_algorithm_serve_time.png")
plot_replication_ci(
  conf_ints = filter(alg$summary_table, metric == "mean_serve_time_nurse"),
  yaxis_title = "Mean time with nurse",
  file_path = path,
  min_rep = alg$nreps[["mean_serve_time_nurse"]]
)
include_graphics(path)
```

```{r}
path <- file.path(output_dir, "reps_algorithm_utilisation.png")
plot_replication_ci(
  conf_ints = filter(alg$summary_table, metric == "utilisation_nurse"),
  yaxis_title = "Mean nurse utilisation",
  file_path = path,
  min_rep = alg$nreps[["utilisation_nurse"]]
)
include_graphics(path)
```

## Explanation of the automated method

This section walks through how the automation code is structured. The algorithm that determines the number of replications is `ReplicationsAlgorithm`. This depends on other R6 classes including `WelfordStats` and `ReplicationTabuliser`.

### WelfordStats

`WelfordStats` is designed to:

* Keep a **running mean and sum of squares**.
* Return **other statistics** based on these (e.g. standard deviation, confidence intervals).
* **Call the `update()`** method of `ReplicationTabuliser` whenever a new data point is processed by `WelfordStats`

#### How do the running mean and sum of squares calculations work?

The running mean and sum of squares are updated iteratively with each new data point provided, **without requiring the storage of all previous data points**. This approach can be referred to as "online" because we only need to store a small set of values (such as the current mean and sum of squares), rather than maintaining an entire list of past values.

For example, focusing on the mean, normally you would need to store all the data points in a list and sum them up to compute the average - for example:

```
data_points <- c(1, 2, 3, 4, 5)
mean <- sum(data_points) / length(data_points)
```

This works fine for small datasets, but as the data grows, maintaining the entire list becomes impractical. Instead, we can update the mean without storing the previous data points using **Welford's online algorithm**. The formula for the running mean is:

$$
\mu_n = \mu_{n-1} + \frac{x_n - \mu_{n-1}}{n}
$$

Where:

- $\mu_n$ is the running mean after the $n$-th data point.
- $x_n$ is the new data point.
- $\mu_{n-1}$ is the running mean before the new data point.

The key thing to notice here is that, to update the mean, **all we needed to know was the current running mean, the new data point, and the number of data points**. A similar formula exists for calculating the sum of squares.

In our code, every time we call `update()` with a new data point, the mean and sum of squares are adjusted, with `n` keeping track of the number of data points so far - for example:

```
WelfordStats <- R6Class("WelfordStats", list( # nolint: object_name_linter

  n = 0L,
  mean = NA,
  ...

  update = function(x) {
    self$n <- self$n + 1L
    ...
      updated_mean <- self$mean + ((x - self$mean) / self$n)
      ...
      self$mean <- updated_meam
      ...
```

#### What other statistics can it calculate?
  
`WelfordStats` then has a series of methods which can return other statistics based on the current mean, sum of squares, and count:

* Variance
* Standard deviation
* Standard error
* Half width of the confidence interval
* Lower confidence interval bound
* Upper confidence interval bound
* Deviation of confidence interval from the mean

### ReplicationTabuliser

`ReplicationTabuliser` keeps track of our results. It:

* Stores **lists with various statistics**, which are updated whenever `update()` is called.
* Can convert these into a **dataframe** using the `summary_table()` method.

![Interaction between WelfordStats and ReplicationTabuliser](../images/replications_statistics.png)

### ReplicationsAlgorithm

The diagram below is a visual representation of the logic in the **ReplicationsAlgorithm**.

Once set up with the relevant parameters, it will first check if there are **initial_replications** to run. These might be specified if the user knows that the model will need at least X amount of replications before any metrics start to get close to the desired precision. The benefit of specifying these is that they are run using **runner()** and so can be run in parallel if chosen.

Once these are run, it checks if any metrics meet precision already. Typically more replications will be required (for the length of the lookahead period) - but if there is no lookahead, they can be marked as solved.

> **What is the lookahead period?**
>
> We want to make sure that the desired precision is stable and maintained for several replications. Here, we refer to this as the lookahead period.
>
> The user will specify **look_ahead** - as noted in [sim-tools](https://tommonks.github.io/sim-tools/04_replications/01_automated_reps.html), this is recommended to be **5** by [Hoad et al. (2010)](https://www.jstor.org/stable/40926090).
>
> The algorithm contains a method **klimit()** which will scale up the lookahead if more than 100 replications have been run, to ensure a sufficient period is being checked for stability, relative to the number of replications. This is simply: `look_ahead/100 * replications`. For example, if we have run 200 replications and look_ahead is 5: `5/100 * 200 = 10`.

After any initial replications, the algorithm enters a while loop. This continues until all metrics are solved or the number of replications surpasses the user-specified **replication_budget** - whichever comes first!

With each loop, it runs the model for another replication, then updates the results for any unsolved metrics from this replication, and checks if precision is met. The **target_met** is a record of how many times in a row precision has been met - once this passes the lookahead period, the metric is marked as solved.

![Visual representation of logic in ReplicationsAlgorithm](../images/replications_algorithm.png)

## Run time

```{r end_timer}
# Get run time in seconds
end_time <- Sys.time()
runtime <- as.numeric(end_time - start_time, units = "secs")

# Display converted to minutes and seconds
minutes <- as.integer(runtime / 60L)
seconds <- as.integer(runtime %% 60L)
cat(sprintf("Notebook run time: %dm %ds", minutes, seconds))
```
